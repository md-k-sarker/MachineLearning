{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Created on Oct 31, 2016\n",
    "\n",
    "@author: sarker\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from random import randint\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    '''\n",
    "    General Neural Network class, with backpropagation.\n",
    "    Activation function: Sigmoid\n",
    "    '''\n",
    "        \n",
    "    def __init__(self,classVals, featuresVals, totalNoOfhiddenLayers, noOfhiddenLayerNeurons):\n",
    "        '''\n",
    "        Initializer for neural network.\n",
    "        Initialize all connections and assign random weight between 0-1 to each connection\n",
    "        \n",
    "        :param classVals:\n",
    "        :param featuresVals:\n",
    "        :param totalNoOfhiddenLayers:\n",
    "        :param noOfhiddenLayerNeurons:\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        WEIGHT is a list of matrix which contains all weight of all connection.\n",
    "        e.g: \n",
    "        For first layer\n",
    "        If input layer no. of neurons is 4+1(bias) and layer 1 no. of neurons is 4 then weight matrix will be\n",
    "        WEIGHT[0] = 4*5 matrix\n",
    "        '''\n",
    "        self.WEIGHT = []\n",
    "        self.neuronsInEachLayer = []\n",
    "        self.totalLayerSize = 0\n",
    "        \n",
    "        InputLayerNoOfNeuronWithBias = len(featuresVals[0]) + 1\n",
    "        HiddenLayerNoOfNeuronWithBias = noOfhiddenLayerNeurons + 1\n",
    "        OutputLayerNoOfNeuron = len(set(classVals))\n",
    "        totalHiddenLayerSize = totalNoOfhiddenLayers    \n",
    "        \n",
    "        '''assign no. of neurons in each layer in self.neuronsInEachayer list'''\n",
    "        self.neuronsInEachLayer.append(InputLayerNoOfNeuronWithBias)\n",
    "        for hl in range(0,totalHiddenLayerSize):\n",
    "            self.neuronsInEachLayer.append(HiddenLayerNoOfNeuronWithBias)\n",
    "        self.neuronsInEachLayer.append(OutputLayerNoOfNeuron) \n",
    "        \n",
    "        print('neurons in each layer', self.neuronsInEachLayer)\n",
    "        \n",
    "        self.totalLayerSize = 1 + totalHiddenLayerSize + 1\n",
    "        \n",
    "        '''assign random weight for input-hiddenLayer 1 connections, i.e. assign random weight to matrix'''\n",
    "        weight = np.zeros((HiddenLayerNoOfNeuronWithBias , InputLayerNoOfNeuronWithBias))\n",
    "        for j in range(0, HiddenLayerNoOfNeuronWithBias):\n",
    "                weightj = np.float128(np.random.uniform(0, 1, size=InputLayerNoOfNeuronWithBias))\n",
    "                weight[j] = weightj\n",
    "        self.WEIGHT.append(weight)\n",
    "        \n",
    "        '''assign random weight for hiddenLayer-hiddenLayer connections'''\n",
    "        for l in range(1, totalHiddenLayerSize):\n",
    "            weight = np.zeros((HiddenLayerNoOfNeuronWithBias , HiddenLayerNoOfNeuronWithBias))\n",
    "            for j in range(0, HiddenLayerNoOfNeuronWithBias):\n",
    "                weightj = np.float128(np.random.uniform(0, 1, size=HiddenLayerNoOfNeuronWithBias))\n",
    "                weight[j] = weightj\n",
    "            self.WEIGHT.append(weight)\n",
    "            \n",
    "        '''assign random weight for hiddenLayer-outputLayer connections'''\n",
    "        weight = np.zeros((OutputLayerNoOfNeuron , HiddenLayerNoOfNeuronWithBias))\n",
    "        for j in range(0, OutputLayerNoOfNeuron):\n",
    "                weightj = np.float128(np.random.uniform(0, 1, size=HiddenLayerNoOfNeuronWithBias))\n",
    "                weight[j] = weightj\n",
    "        self.WEIGHT.append(weight)\n",
    "        \n",
    "\n",
    "    def trainModel(self,inputVector, outputVector, maxIteration, minError, learningRate=.3, momentum=50, batch = False):\n",
    "        '''\n",
    "        Train Neural network using backpropagation.\n",
    "        Currently online update is implemented.\n",
    "        To do impementation: \n",
    "        Batch Mode. -- \n",
    "        1. Stochastic gradient descent ---- Choose any data point randomly and calculate error.\n",
    "        2. Traditional gradient descent --- Iterate over all data point to calculate error.  \n",
    "        :param inputVector:\n",
    "        :param outputVector:\n",
    "        :param maxIteration:\n",
    "        :param minError:\n",
    "        :param learningRate:\n",
    "        :param momentum:\n",
    "        :param batch:\n",
    "        '''\n",
    "\n",
    "        avgCostPerIteration = 1e10\n",
    "        costsPerIteration = []\n",
    "        iteration = 0\n",
    "          \n",
    "        while(iteration < maxIteration and avgCostPerIteration > minError):\n",
    "            costPerIteration = 0\n",
    "            iteration += 1\n",
    "            finalLayerOutputs = []\n",
    "            \n",
    "            for i in range(0, inputVector.shape[0]):\n",
    "                '''Forward Propagation'''\n",
    "                '''List of output for each layer'''\n",
    "                OUTPUT = []\n",
    "                '''List of error/delta for each layer'''\n",
    "                DELTA = []\n",
    "                \n",
    "                input = np.float128(np.ones(len(inputVector[i])+1))\n",
    "                '''add bias term as 0.9'''\n",
    "                input[0] = 0.9\n",
    "                input[1:len(inputVector[i])+1] = inputVector[i]\n",
    "                output = input\n",
    "                OUTPUT.append(output)\n",
    "                \n",
    "                '''calculate output for each layer'''\n",
    "                for l,W in zip(range(1,self.totalLayerSize),self.WEIGHT):\n",
    "                    input = np.dot(W, OUTPUT[l-1])\n",
    "                    output = np.array([ gerReLu(val) for val in input])\n",
    "                    OUTPUT.append(output)\n",
    "                \n",
    "                                   \n",
    "                '''calculate cost. Cost is calculated on output layer.'''\n",
    "                costPerInput = 0\n",
    "                for j in range(0, self.neuronsInEachLayer[len(self.neuronsInEachLayer)-1]):\n",
    "                    costPerInput += -outputVector[i][j] * getLog(OUTPUT[len(OUTPUT)-1][j]) - (1 - outputVector[i][j]) * (get1MinusThetaLog(OUTPUT[len(OUTPUT)-1][j]))\n",
    "                \n",
    "                '''divide by no of. neurons in output layer\n",
    "                to get cost for single output neuron.'''\n",
    "                costPerIteration += costPerInput / len(outputVector[i])\n",
    "                \n",
    "                \n",
    "                '''Backward Propagation'''\n",
    "                '''error in output layer'''\n",
    "                error = outputVector[i] - OUTPUT[len(OUTPUT)-1]\n",
    "                delta = OUTPUT[len(OUTPUT)-1] * (1 - OUTPUT[len(OUTPUT)-1]) * error[np.newaxis]\n",
    "                DELTA.append(delta)\n",
    "                \n",
    "                '''error in hidden layer'''\n",
    "                for W, ol in zip(reversed(self.WEIGHT[1:]), range(len(OUTPUT) - 2,-1,-1)):\n",
    "                    error = np.dot (W.T , error)\n",
    "                    delta = OUTPUT[ol] * (1- OUTPUT[ol]) * error[np.newaxis]\n",
    "                    DELTA.append(delta)\n",
    "                    \n",
    "                \n",
    "                '''weight update'''\n",
    "                if( batch == False):\n",
    "                    '''here output of previous layer. that means input of that layer.'''\n",
    "                    for wl, OT, DT in zip(range(len(self.WEIGHT)-1,-1,-1), reversed(OUTPUT[:len(OUTPUT)-1]), DELTA):\n",
    "                        self.WEIGHT[wl] = self.WEIGHT[wl] + learningRate * OT * DT.T \n",
    "                \n",
    "                finalLayerOutputs.append(output)\n",
    "            \n",
    "                \n",
    "            '''average over inputs'''\n",
    "            avgCostPerIteration = costPerIteration / len(inputVector)\n",
    "\n",
    "            costsPerIteration.append(avgCostPerIteration)\n",
    "            '''print cost per iteration to show the error is decreasing'''\n",
    "            sampleNo = randint(0,inputVector.shape[0]-1)\n",
    "            if(iteration % 100 == 0):\n",
    "                print('IterationNo: ', iteration, ' CostPerIteration: ', avgCostPerIteration, ' ActualOutput[',sampleNo,']: ', outputVector[sampleNo], ' PredictedOutput: ',finalLayerOutputs[sampleNo])\n",
    "    \n",
    "        \n",
    "        return self.WEIGHT, costsPerIteration\n",
    "    \n",
    "\n",
    "    def testModel(self,inputVector, outputVector, Weight):\n",
    "        '''\n",
    "        Test predicted classes.\n",
    "        \n",
    "        :param inputVector:\n",
    "        :param outputVector:\n",
    "        :param Weight:\n",
    "        '''\n",
    "        \n",
    "        predictedClasses = []\n",
    "        \n",
    "        for i in range(0, inputVector.shape[0]):\n",
    "            \n",
    "            input = np.ones(self.neuronsInEachLayer[0])\n",
    "            input[1:len(inputVector[0]) + 1] = inputVector[i]\n",
    "    \n",
    "            #for layer 1\n",
    "            output = input\n",
    "            \n",
    "            '''for layer 1 to output layer'''\n",
    "            for layer in range(1,len(Weight) +1):\n",
    "                input = np.dot(Weight[layer-1] , output)\n",
    "                output = np.array([ gerReLu(val) for val in input])\n",
    "                \n",
    "            predictedClasses.append(output)     \n",
    "        \n",
    "        return np.array(predictedClasses)\n",
    "\n",
    "    \n",
    "def getLog(x):\n",
    "    return np.float128(math.log(x))    \n",
    "    \n",
    "def get1MinusThetaLog(x):\n",
    "    return  np.float128(math.log(1-x))\n",
    "    \n",
    "def getSigmoid(x):\n",
    "    try:\n",
    "        denom = np.float128(math.exp(-x))\n",
    "        denomWithOnePlus = np.float128(denom + 1.0) \n",
    "    except:\n",
    "        '''math domain error or math range error occurred'''\n",
    "            \n",
    "    return np.float128(1 / denomWithOnePlus)\n",
    "\n",
    "def gerReLu(x):\n",
    "    return max(0,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
